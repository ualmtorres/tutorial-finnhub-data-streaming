<!DOCTYPE html>
<html lang="es">
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 2.0.17">
<meta name="author" content="Máster en Tecnologías y Aplicaciones en Ingeniería Informática. Universidad de Almería -">
<title>Desarrollo de un cuadro de mandos para Finnhub Stock</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
<style>
/*! Asciidoctor default stylesheet | MIT License | https://asciidoctor.org */
/* Uncomment the following line when using as a custom stylesheet */
/* @import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700"; */
html{font-family:sans-serif;-webkit-text-size-adjust:100%}
a{background:none}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
b,strong{font-weight:bold}
abbr{font-size:.9em}
abbr[title]{cursor:help;border-bottom:1px dotted #dddddf;text-decoration:none}
dfn{font-style:italic}
hr{height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
audio,video{display:inline-block}
audio:not([controls]){display:none;height:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type=button],input[type=reset],input[type=submit]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type=checkbox],input[type=radio]{padding:0}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,::before,::after{box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;line-height:1;position:relative;cursor:auto;-moz-tab-size:4;-o-tab-size:4;tab-size:4;word-wrap:anywhere;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:0}
p{line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #dddddf;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0}
ul.square li ul,ul.circle li ul,ul.disc li ul{list-style:inherit}
ul.square{list-style-type:square}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:1px solid #dedede;word-wrap:normal}
table thead,table tfoot{background:#f7f8f7}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt{background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.center{margin-left:auto;margin-right:auto}
.stretch{width:100%}
.clearfix::before,.clearfix::after,.float-group::before,.float-group::after{content:" ";display:table}
.clearfix::after,.float-group::after{clear:both}
:not(pre).nobreak{word-wrap:normal}
:not(pre).nowrap{white-space:nowrap}
:not(pre).pre-wrap{white-space:pre-wrap}
:not(pre):not([class^=L])>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background:#f7f7f8;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed}
pre{color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;line-height:1.45;text-rendering:optimizeSpeed}
pre code,pre pre{color:inherit;font-size:inherit;line-height:inherit}
pre>code{display:block}
pre.nowrap,pre.nowrap pre{white-space:pre;word-wrap:normal}
em em{font-style:normal}
strong strong{font-weight:400}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background:#f7f7f7;border:1px solid #ccc;border-radius:3px;box-shadow:0 1px 0 rgba(0,0,0,.2),inset 0 0 0 .1em #fff;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menuref{color:#000}
.menuseq b:not(.caret),.menuref{font-weight:inherit}
.menuseq{word-spacing:-.02em}
.menuseq b.caret{font-size:1.25em;line-height:.8}
.menuseq i.caret{font-weight:bold;text-align:center;width:.45em}
b.button::before,b.button::after{position:relative;top:-1px;font-weight:400}
b.button::before{content:"[";padding:0 3px 0 2px}
b.button::after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin:0 auto;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header::before,#header::after,#content::before,#content::after,#footnotes::before,#footnotes::after,#footer::before,#footer::after{content:" ";display:table}
#header::after,#content::after,#footnotes::after,#footer::after{clear:both}
#content{margin-top:1.25em}
#content::before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #dddddf}
#header>h1:only-child,body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #dddddf;padding-bottom:8px}
#header .details{border-bottom:1px solid #dddddf;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:flex;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span::before{content:"\00a0\2013\00a0"}
#header .details br+span.author::before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark::before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber::after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #dddddf;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #e7e7e9;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
#toc.toc2{margin-top:0!important;background:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #e7e7e9;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #e7e7e9;left:auto;right:0}}
@media screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border:1px solid #e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:none;background:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:hsla(0,0%,100%,.8);line-height:1.44}
#content{margin-bottom:.625em}
.sect1{padding-bottom:.625em}
@media screen and (min-width:768px){#content{margin-bottom:1.25em}
.sect1{padding-bottom:1.25em}}
.sect1:last-child{padding-bottom:0}
.sect1+.sect1{border-top:1px solid #e7e7e9}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor::before,h2>a.anchor::before,h3>a.anchor::before,#toctitle>a.anchor::before,.sidebarblock>.content>.title>a.anchor::before,h4>a.anchor::before,h5>a.anchor::before,h6>a.anchor::before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
details,.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
details{margin-left:1.25rem}
details>summary{cursor:pointer;display:block;position:relative;line-height:1.6;margin-bottom:.625rem;outline:none;-webkit-tap-highlight-color:transparent}
details>summary::-webkit-details-marker{display:none}
details>summary::before{content:"";border:solid transparent;border-left:solid;border-width:.3em 0 .3em .5em;position:absolute;top:.5em;left:-1.25rem;transform:translateX(15%)}
details[open]>summary::before{border:solid transparent;border-top:solid;border-width:.5em .3em 0;transform:translateY(15%)}
details>summary::after{content:"";width:1.25rem;height:1em;position:absolute;top:.3em;left:-1.25rem}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock.fit-content>caption.title{white-space:nowrap;width:0}
.paragraph.lead>p,#preamble>.sectionbody>[class=paragraph]:first-of-type p{font-size:1.21875em;line-height:1.6;color:rgba(0,0,0,.85)}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #dddddf;color:rgba(0,0,0,.6);word-wrap:anywhere}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border:1px solid #e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;border-radius:4px}
.exampleblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child{margin-bottom:0}
.sidebarblock{border:1px solid #dbdbd6;margin-bottom:1.25em;padding:1.25em;background:#f3f3f2;border-radius:4px}
.sidebarblock>:first-child{margin-top:0}
.sidebarblock>:last-child{margin-bottom:0}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock>.content>pre{border-radius:4px;overflow-x:auto;padding:1em;font-size:.8125em}
@media screen and (min-width:768px){.literalblock pre,.listingblock>.content>pre{font-size:.90625em}}
@media screen and (min-width:1280px){.literalblock pre,.listingblock>.content>pre{font-size:1em}}
.literalblock pre,.listingblock>.content>pre:not(.highlight),.listingblock>.content>pre[class=highlight],.listingblock>.content>pre[class^="highlight "]{background:#f7f7f8}
.literalblock.output pre{color:#f7f7f8;background:rgba(0,0,0,.9)}
.listingblock>.content{position:relative}
.listingblock code[data-lang]::before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:inherit;opacity:.5}
.listingblock:hover code[data-lang]::before{display:block}
.listingblock.terminal pre .command::before{content:attr(data-prompt);padding-right:.5em;color:inherit;opacity:.5}
.listingblock.terminal pre .command:not([data-prompt])::before{content:"$"}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.prettyprint{background:#f7f7f8}
pre.prettyprint .linenums{line-height:1.45;margin-left:2em}
pre.prettyprint li{background:none;list-style-type:inherit;padding-left:0}
pre.prettyprint li code[data-lang]::before{opacity:1}
pre.prettyprint li:not(:first-child) code[data-lang]::before{display:none}
table.linenotable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.linenotable td[class]{color:inherit;vertical-align:top;padding:0;line-height:inherit;white-space:normal}
table.linenotable td.code{padding-left:.75em}
table.linenotable td.linenos,pre.pygments .linenos{border-right:1px solid;opacity:.35;padding-right:.5em;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}
pre.pygments span.linenos{display:inline-block;margin-right:.75em}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock:not(.excerpt)>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote::before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.75em;margin-right:.5ex;text-align:right}
.verseblock{margin:0 1em 1.25em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans-serif;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract blockquote::before,.quoteblock.excerpt blockquote::before,.quoteblock .quoteblock blockquote::before{display:none}
.quoteblock.abstract blockquote,.quoteblock.abstract p,.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{line-height:1.6;word-spacing:0}
.quoteblock.abstract{margin:0 1em 1.25em;display:block}
.quoteblock.abstract>.title{margin:0 0 .375em;font-size:1.15em;text-align:center}
.quoteblock.excerpt>blockquote,.quoteblock .quoteblock{padding:0 0 .25em 1em;border-left:.25em solid #dddddf}
.quoteblock.excerpt,.quoteblock .quoteblock{margin-left:0}
.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{color:inherit;font-size:1.0625rem}
.quoteblock.excerpt .attribution,.quoteblock .quoteblock .attribution{color:inherit;font-size:.85rem;text-align:left;margin-right:0}
p.tableblock:last-child{margin-bottom:0}
td.tableblock>.content{margin-bottom:1.25em;word-wrap:anywhere}
td.tableblock>.content>:last-child{margin-bottom:-1.25em}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>*>tr>*{border-width:1px}
table.grid-cols>*>tr>*{border-width:0 1px}
table.grid-rows>*>tr>*{border-width:1px 0}
table.frame-all{border-width:1px}
table.frame-ends{border-width:1px 0}
table.frame-sides{border-width:0 1px}
table.frame-none>colgroup+*>:first-child>*,table.frame-sides>colgroup+*>:first-child>*{border-top-width:0}
table.frame-none>:last-child>:last-child>*,table.frame-sides>:last-child>:last-child>*{border-bottom-width:0}
table.frame-none>*>tr>:first-child,table.frame-ends>*>tr>:first-child{border-left-width:0}
table.frame-none>*>tr>:last-child,table.frame-ends>*>tr>:last-child{border-right-width:0}
table.stripes-all>*>tr,table.stripes-odd>*>tr:nth-of-type(odd),table.stripes-even>*>tr:nth-of-type(even),table.stripes-hover>*>tr:hover{background:#f8f8f7}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.checklist,ul.none,ol.none,ul.no-bullet,ol.no-bullet,ol.unnumbered,ul.unstyled,ol.unstyled{list-style-type:none}
ul.no-bullet,ol.no-bullet,ol.unnumbered{margin-left:.625em}
ul.unstyled,ol.unstyled{margin-left:0}
li>p:empty:only-child::before{content:"";display:inline-block}
ul.checklist>li>p:first-child{margin-left:-1em}
ul.checklist>li>p:first-child>.fa-square-o:first-child,ul.checklist>li>p:first-child>.fa-check-square-o:first-child{width:1.25em;font-size:.8em;position:relative;bottom:.125em}
ul.checklist>li>p:first-child>input[type=checkbox]:first-child{margin-right:.25em}
ul.inline{display:flex;flex-flow:row wrap;list-style:none;margin:0 0 .625em -1.25em}
ul.inline>li{margin-left:1.25em}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
td.hdlist2{word-wrap:anywhere}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist td:not([class]):first-child{padding:.4em .75em 0;line-height:1;vertical-align:top}
.colist td:not([class]):first-child img{max-width:none}
.colist td:not([class]):last-child{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:4px solid #fff;box-shadow:0 0 0 1px #ddd}
.imageblock.left{margin:.25em .625em 1.25em 0}
.imageblock.right{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em;border-width:1px 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none;margin-left:-1.05em}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background:#00fafa}
.black{color:#000}
.black-background{background:#000}
.blue{color:#0000bf}
.blue-background{background:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background:#fa00fa}
.gray{color:#606060}
.gray-background{background:#7d7d7d}
.green{color:#006000}
.green-background{background:#007d00}
.lime{color:#00bf00}
.lime-background{background:#00fa00}
.maroon{color:#600000}
.maroon-background{background:#7d0000}
.navy{color:#000060}
.navy-background{background:#00007d}
.olive{color:#606000}
.olive-background{background:#7d7d00}
.purple{color:#600060}
.purple-background{background:#7d007d}
.red{color:#bf0000}
.red-background{background:#fa0000}
.silver{color:#909090}
.silver-background{background:#bcbcbc}
.teal{color:#006060}
.teal-background{background:#007d7d}
.white{color:#bfbfbf}
.white-background{background:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background:#fafa00}
span.icon>.fa{cursor:default}
a span.icon>.fa{cursor:inherit}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note::before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip::before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning::before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution::before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important::before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background:rgba(0,0,0,.8);border-radius:50%;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]::after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt,summary{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,span.alt,summary{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background:#fffef7;border-color:#e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@page{margin:1.25cm .75cm}
@media print{*{box-shadow:none!important;text-shadow:none!important}
html{font-size:80%}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare)::after,a[href^="https:"]:not(.bare)::after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]{border-bottom:1px dotted}
abbr[title]::after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#header,#content,#footnotes,#footer{max-width:none}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #dddddf!important;padding-bottom:0!important}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span::before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]::before{display:block}
#footer{padding:0 .9375em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
@media amzn-kf8,print{#header>h1:first-child{margin-top:1.25rem}
.sect1{padding:0!important}
.sect1+.sect1{border:0}
#footer{background:none}
#footer-text{color:rgba(0,0,0,.6);font-size:.9em}}
@media amzn-kf8{#header,#content,#footnotes,#footer{padding:0}}
</style>
</head>
<body class="book toc2 toc-right">
<div id="header">
<h1>Desarrollo de un cuadro de mandos para Finnhub Stock</h1>
<div class="details">
<span id="author" class="author">Máster en Tecnologías y Aplicaciones en Ingeniería Informática. Universidad de Almería -</span><br>
<span id="revdate">José Antonio Martínez &lt;jmartine@ual.es&gt; y Manuel Torres &lt;mtorres@ual.es&gt;</span>
</div>
<div id="toc" class="toc2">
<div id="toctitle">Tabla de contenidos</div>
<ul class="sectlevel1">
<li><a href="#_resumen">Resumen</a></li>
<li><a href="#_introducción">1. Introducción</a></li>
<li><a href="#_configuración_de_la_plataforma">2. Configuración de la plataforma</a>
<ul class="sectlevel2">
<li><a href="#_creación_de_la_máquina_virtual_openstack_con_terraform">2.1. Creación de la máquina virtual OpenStack con Terraform</a></li>
<li><a href="#_creación_de_la_plataforma_de_data_streaming_y_visualización_con_docker">2.2. Creación de la plataforma de data streaming y visualización con Docker</a></li>
<li><a href="#_configuración_inicial_de_la_plataforma">2.3. Configuración inicial de la plataforma</a></li>
</ul>
</li>
<li><a href="#_desarrollo_de_la_plataforma">3. Desarrollo de la plataforma</a>
<ul class="sectlevel2">
<li><a href="#_creación_de_un_productor_de_datos_de_finnhub_stock">3.1. Creación de un productor de datos de Finnhub Stock</a></li>
<li><a href="#_creación_de_un_consumidor_de_datos_de_kafka">3.2. Creación de un consumidor de datos de Kafka</a></li>
<li><a href="#_creación_de_un_componente_spark_streaming">3.3. Creación de un componente Spark Streaming</a></li>
<li><a href="#_visualización_de_los_datos_en_grafana">3.4. Visualización de los datos en Grafana</a></li>
</ul>
</li>
<li><a href="#_conclusiones">4. Conclusiones</a></li>
</ul>
</div>
</div>
<div id="content">
<div id="preamble">
<div class="sectionbody">
<div class="imageblock">
<div class="content">
<img src="images/di.png" alt="di">
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_resumen">Resumen</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Finnhub Stock es una API que proporciona datos de acciones en tiempo real, noticias financieras y más. Es una herramienta útil para desarrolladores que desean crear aplicaciones financieras o realizar análisis de datos. En este tutorial vamos a ver cómo crear una plataforma alrededor de la API de Finnhub Stock. La plataforma ofrecerá un cuadro de mandos con información en tiempo real sobre las acciones bursátiles de una empresa. Los datos obtenidos de la API de Finnhub Stock se enviarán a un servidor Kafka para su procesamiento posterior mediante Spark Streaming. Los datos procesados se almacenarán en una base de datos Cassandra para su consulta y visualización en un cuadro de mandos en Grafana. Con estos componentes, podremos crear una plataforma completa para visualizar los precios en tiempo real de las acciones de una empresa. La plataforma nos permitirá realizar análisis de datos sobre las acciones y tomar decisiones informadas sobre inversiones financieras. La plataforma se instalará en un entorno de pruebas con Docker y se ejecutará sobre una máquina virtual OpenStack, la cual crearemos con Terraform.</p>
</div>
<div class="ulist">
<div class="title">Objetivos</div>
<ul>
<li>
<p>Aprender a utilizar la API de Finnhub Stock para obtener datos de acciones en tiempo real.</p>
</li>
<li>
<p>Desarrollar una plataforma de data streaming y visualización de Finnhub Stock con Apache Kafka, Apache Spark, Cassandra y Grafana.</p>
</li>
<li>
<p>Crear un cuadro de mandos interactivo con Grafana para visualizar los precios en tiempo real de las acciones de una empresa.</p>
</li>
<li>
<p>Destacar la importancia de la infraestructura como código (IaC) para despliegues automatizados.</p>
</li>
</ul>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
<div class="paragraph">
<p>Disponibles en GitHub los siguientes repositorios:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="https://github.com/ualmtorres/finnhub-data-streaming-terraform">Repositorio de creación de la máquina virtual OpenStack con Terraform</a></p>
</li>
<li>
<p><a href="https://github.com/ualmtorres/finnhub-data-streaming-docker-compose">Repositorio de la plataforma de data streaming y visualización con Docker</a></p>
</li>
<li>
<p><a href="https://github.com/ualmtorres/finnhub-data-streaming-workload-simulator">Repositorio de los productores, consumidores y procesadores de datos de Finnhub Stock</a></p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_introducción">1. Introducción</h2>
<div class="sectionbody">
<div class="paragraph">
<p><a href="https://finnhub.io/">Finnhub Stock</a> es una API que proporciona datos de acciones en tiempo real, noticias financieras y más. Es una herramienta útil para desarrolladores que desean crear aplicaciones financieras o realizar análisis de datos. Actualmente, Finnhub Stock ofrece una API gratuita con una amplia gama de funcionalidades. Entre estas funcionalidades se incluyen la obtención de precios en tiempo real, datos históricos, información sobre empresas y más. Algunas de estas funcionalidades están disponibles en el plan gratuito, mientras que otras requieren una suscripción de pago. Los servicios están disponibles como API REST y Websocket, lo que permite a los desarrolladores elegir la forma de acceder a los datos. La API de Finnhub Stock es fácil de usar y proporciona una gran cantidad de datos para ayudarte a desarrollar tus aplicaciones financieras.</p>
</div>
<div class="paragraph">
<p>En este tutorial vamos a ver cómo crear una plataforma alrededor de la API de Finnhub Stock. La plataforma creada ofrecerá un cuadro de mandos con información en tiempo real sobre las acciones de una empresa. Para ello, desarrollaremos los siguientes componentes:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Un componente para consumir de la API de Finnhub Stock y obtener los datos de las acciones de una empresa. Este componente realizará solicitudes Websocket a la API para obtener los precios en tiempo real de las acciones. Los datos serán enviados a un servidor Kafka para su procesamiento posterior. Este componente actuará como <em>productor</em> de datos a la plataforma.</p>
</li>
<li>
<p>Un componente Spark Streaming para procesar los datos de las acciones en tiempo real. Este componente leerá los datos del servidor Kafka y realizará cálculos sobre los precios de las acciones.</p>
</li>
<li>
<p>Una base de datos para almacenar los datos procesados por el componente Spark Streaming. Utilizaremos una base de datos NoSQL como Cassandra para almacenar los datos de las acciones. Los datos se almacenarán en tablas que permitan realizar consultas eficientes sobre los precios de las acciones. Esta base de datos será el repositorio de datos de la plataforma.</p>
</li>
<li>
<p>Un cuadro de mandos en Grafana para visualizar los datos de las acciones de una empresa. Utilizaremos Grafana para crear gráficos interactivos que muestren los precios en tiempo real de las acciones. Los gráficos se actualizarán automáticamente a medida que se reciban nuevos datos de la API de Finnhub Stock.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>La figura siguiente muestra la arquitectura de la plataforma que vamos a desarrollar.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/architecture.png" alt="Arquitectura de la plataforma">
</div>
</div>
<div class="paragraph">
<p>Con estos componentes, podremos crear una plataforma completa para visualizar los precios en tiempo real de las acciones de una empresa. La plataforma nos permitirá realizar análisis de datos sobre las acciones y tomar decisiones informadas sobre inversiones financieras.</p>
</div>
<div class="paragraph">
<p>La figura siguiente muestra un ejemplo del cuadro de mandos que vamos a crear con Grafana. En el cuadro de mandos, podremos ver los precios en tiempo real de las acciones de una empresa. Los gráficos se actualizarán automáticamente a medida que se reciban nuevos datos de la API de Finnhub Stock y se procesen en la plataforma.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/dashboard.png" alt="Cuadro de mandos de Grafana">
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_configuración_de_la_plataforma">2. Configuración de la plataforma</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Para configurar la plataforma, necesitaremos instalar y configurar los siguientes componentes:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Apache Kafka: Utilizaremos <a href="https://kafka.apache.org/">Apache Kafka</a> como servidor de mensajería para guardar los datos recibidos de la API de Finnhub Stock y enviar los datos de las acciones al componente Spark Streaming.</p>
</li>
<li>
<p>Apache Spark: Utilizaremos <a href="https://spark.apache.org/">Apache Spark</a> para procesar los datos de las acciones en tiempo real y realizar cálculos sobre los precios de las acciones.</p>
</li>
<li>
<p>Apache Cassandra: Utilizaremos <a href="https://cassandra.apache.org/_/index.html">Apache Cassandra</a> como base de datos NoSQL para almacenar los datos de las acciones procesados por el componente Spark Streaming.</p>
</li>
<li>
<p>Grafana: Utilizaremos <a href="https://grafana.com/">Grafana</a> para crear un cuadro de mandos interactivo que muestre los precios en tiempo real de las acciones de una empresa.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>En los siguientes pasos, veremos cómo instalar y configurar cada uno de estos componentes en nuestra plataforma.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>Los scripts del tutorial para producir, consumir y procesar datos están programdos en Python.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Esta plataforma se instalará en un entorno de pruebas con Docker. Dicha plataforma se ejecutará sobre una máquina virtual en un cloud OpenStack, la cual crearemos con Infraestructura como Código (IaC) con Terraform.</p>
</div>
<div class="sect2">
<h3 id="_creación_de_la_máquina_virtual_openstack_con_terraform">2.1. Creación de la máquina virtual OpenStack con Terraform</h3>
<div class="paragraph">
<p><a href="https://www.terraform.io/">Terraform</a> es una herramienta de código abierto que permite definir y configurar la infraestructura mediante programación, con código. Con Terraform, podemos definir nuestra infraestructura como código y desplegarla de forma automatizada. Para más información sobre Terraform, puedes consultar este <a href="https://ualmtorres.github.io/SeminarioTerraform/">tutorial sobre despliegue de infraestructura con Terraform</a>.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>Para instalar Terraform, puedes seguir las instrucciones de la <a href="https://learn.hashicorp.com/tutorials/terraform/install-cli">documentación oficial de Terraform</a>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>En este caso, vamos a utilizar Terraform para crear una máquina virtual en OpenStack. La máquina virtual será el lugar donde desplegaremos nuestra plataforma de data streaming y visualización de Finnhub Stock. Cabe destacar lo siguiente:</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>El código de Terraform se encuentra en este <a href="https://github.com/ualmtorres/finnhub-data-streaming-terraform">repositorio de GitHub</a>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="ulist">
<ul>
<li>
<p>Archivo <a href="https://github.com/ualmtorres/finnhub-data-streaming-terraform/blob/main/variables.tf"><code>variables.tf</code></a>: Define las variables necesarias para la configuración de la máquina virtual, como el nombre de la máquina, la imagen base, el sabor, la red, etc.</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-terraform" data-lang="terraform">variable "openstack_user_name" {} <b class="conum">(1)</b>
variable "openstack_tenant_name" {} <b class="conum">(2)</b>
variable "PASSWORD" {} <b class="conum">(3)</b>
variable "openstack_auth_url" {} <b class="conum">(4)</b>
variable "openstack_keypair" {} <b class="conum">(5)</b>
variable "cidr" {} <b class="conum">(6)</b>

variable "image_name" {} <b class="conum">(7)</b>
variable "availability_zone" {} <b class="conum">(8)</b>
variable "flavor_name" {} <b class="conum">(9)</b>
variable "network_name" {} <b class="conum">(10)</b>
variable "floating_ip" {}  <b class="conum">(11)</b>

variable "openstack_private_key_file" {} <b class="conum">(12)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Nombre de usuario de OpenStack.</p>
</li>
<li>
<p>Nombre del proyecto de OpenStack donde se creará la máquina virtual.</p>
</li>
<li>
<p>Contraseña del usuario de OpenStack.</p>
</li>
<li>
<p>URL de autenticación de OpenStack.</p>
</li>
<li>
<p>Nombre del par de claves de OpenStack (contiene la clave pública).</p>
</li>
<li>
<p>Rango de direcciones IP para la red de la máquina virtual.</p>
</li>
<li>
<p>Nombre de la imagen base de la máquina virtual.</p>
</li>
<li>
<p>Zona de disponibilidad de la máquina virtual.</p>
</li>
<li>
<p>Sabor de la máquina virtual (tamaño de la instancia).</p>
</li>
<li>
<p>Nombre de la red donde se conectará la máquina virtual.</p>
</li>
<li>
<p>Dirección IP flotante para la máquina virtual.</p>
</li>
<li>
<p>Ruta al archivo local de clave privada para conectarse a la máquina virtual.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Archivo <a href="https://github.com/ualmtorres/finnhub-data-streaming-terraform/blob/main/terraform.tfvars.example"><code>terraform.tfvars</code></a>: Define los valores de las variables necesarias para la configuración de la máquina virtual.</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-terraform" data-lang="terraform">openstack_user_name   = "********"
openstack_tenant_name = "********"
openstack_auth_url    = "********"
openstack_keypair     = "********"
cidr                  = "********"

image_name            = "********"
availability_zone     = "********"
flavor_name           = "********" <b class="conum">(1)</b>
network_name          = "********"
floating_ip           = "********"

openstack_private_key_file = "********"</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>El tamaño de la instancia será de al menos 32 GB de RAM y 8 CPUs por la cantidad de servicios que se van a ejecutar y las necesidades de Spark Streaming.</p>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>No se ha incluido la contraseña de OpenStack en el archivo <code>terraform.tfvars</code> por motivos de seguridad. Se puede definir la contraseña como una variable de entorno o introducirla manualmente al ejecutar Terraform.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Archivo <a href="https://github.com/ualmtorres/finnhub-data-streaming-terraform/blob/main/providers.tf"><code>provider.tf</code></a>: Define el proveedor de OpenStack y las credenciales necesarias para autenticarse en OpenStack.</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-terraform" data-lang="terraform">terraform {
  required_version = "&gt;= 0.14.0"
  required_providers {
    openstack = {
      source  = "terraform-provider-openstack/openstack"
      version = "~&gt; 1.53.0"
    }
  }
}
provider "openstack" {
  user_name   = var.openstack_user_name
  tenant_name = var.openstack_tenant_name
  password    = var.PASSWORD
  auth_url    = var.openstack_auth_url
}</code></pre>
</div>
</div>
</li>
<li>
<p>Archivo <a href="https://github.com/ualmtorres/finnhub-data-streaming-terraform/blob/main/security-groups.tf"><code>security-groups.tf</code></a>: Define el grupo de seguridad de OpenStack y las reglas necesarias para acceder a los componentes expuestos de la plataforma.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>Se necesitan reglas de seguridad al menos para los puertos 22 (SSH), 443 (HTTPS), 19000 (Kafdrop), 4000 (Cassandra Web), 3000 (Grafana) y 8080 (Spark Master).</p>
</div>
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-terraform" data-lang="terraform"># Create tradedataprocessing security group
resource "openstack_networking_secgroup_v2" "tradedataprocessing" {
  name        = "tradedataprocessing"
  description = "data processing security group"
}

resource "openstack_networking_secgroup_rule_v2" "ssh" {
  description       = "SSH"
  direction         = "ingress"
  ethertype         = "IPv4"
  protocol          = "tcp"
  port_range_max    = 22
  port_range_min    = 22
  security_group_id = openstack_networking_secgroup_v2.tradedataprocessing.id
}

resource "openstack_networking_secgroup_rule_v2" "https" {
  description       = "HTTPS"
  direction         = "ingress"
  ethertype         = "IPv4"
  protocol          = "tcp"
  port_range_max    = 443
  port_range_min    = 443
  security_group_id = openstack_networking_secgroup_v2.tradedataprocessing.id
}

resource "openstack_networking_secgroup_rule_v2" "kafdrop" {
  description       = "kafdrop"
  direction         = "ingress"
  ethertype         = "IPv4"
  protocol          = "tcp"
  port_range_max    = 19000
  port_range_min    = 19000
  security_group_id = openstack_networking_secgroup_v2.tradedataprocessing.id
}

resource "openstack_networking_secgroup_rule_v2" "cassandraweb" {
  description       = "cassandraweb"
  direction         = "ingress"
  ethertype         = "IPv4"
  protocol          = "tcp"
  port_range_max    = 4000
  port_range_min    = 4000
  security_group_id = openstack_networking_secgroup_v2.tradedataprocessing.id
}

resource "openstack_networking_secgroup_rule_v2" "grafana" {
  description       = "grafana"
  direction         = "ingress"
  ethertype         = "IPv4"
  protocol          = "tcp"
  port_range_max    = 3000
  port_range_min    = 3000
  security_group_id = openstack_networking_secgroup_v2.tradedataprocessing.id
}

resource "openstack_networking_secgroup_rule_v2" "sparkmaster" {
  description       = "sparkmaster"
  direction         = "ingress"
  ethertype         = "IPv4"
  protocol          = "tcp"
  port_range_max    = 8080
  port_range_min    = 8080
  security_group_id = openstack_networking_secgroup_v2.tradedataprocessing.id
}

resource "openstack_networking_secgroup_rule_v2" "sparkhistoryserver" {
  description       = "sparkhistoryserver"
  direction         = "ingress"
  ethertype         = "IPv4"
  protocol          = "tcp"
  port_range_max    = 18080
  port_range_min    = 18080
  security_group_id = openstack_networking_secgroup_v2.tradedataprocessing.id
}

resource "openstack_networking_secgroup_rule_v2" "hadoopjobhistory" {
  description       = "hadoopjobhistory"
  direction         = "ingress"
  ethertype         = "IPv4"
  protocol          = "tcp"
  port_range_max    = 19888
  port_range_min    = 19888
  security_group_id = openstack_networking_secgroup_v2.tradedataprocessing.id
}</code></pre>
</div>
</div>
</li>
<li>
<p>Archivo <a href="https://github.com/ualmtorres/finnhub-data-streaming-terraform/blob/main/main.tf"><code>main.tf</code></a>: Define los recursos de Terraform necesarios para crear la máquina virtual en OpenStack.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>La máquina virtual creada se aprovisionará con Docker y otros componentes necesarios para instalar la plataforma de data streaming y visualización de Finnhub Stock. Esto se hará mediante un script de inicialización que se ejecutará al crear la máquina virtual.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-terraform" data-lang="terraform">resource "openstack_compute_instance_v2" "tradedataprocessing_instance" {
  name              = "tradedataprocessing"
  image_name        = var.image_name
  availability_zone = var.availability_zone
  flavor_name       = var.flavor_name
  key_pair          = var.openstack_keypair
  security_groups   = [openstack_networking_secgroup_v2.tradedataprocessing.id]
  network {
    name = var.network_name
  }

  user_data = file("tradedataprocessing-setup.sh") <b class="conum">(1)</b>
}

resource "openstack_compute_floatingip_associate_v2" "ip_assoc" {
  floating_ip = var.floating_ip
  instance_id = openstack_compute_instance_v2.tradedataprocessing_instance.id

  depends_on = [
    openstack_compute_instance_v2.tradedataprocessing_instance
  ]
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Archivo <code>tradedataprocessing-setup.sh</code>: Script de inicialización de la máquina virtual.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Archivo <a href="https://github.com/ualmtorres/finnhub-data-streaming-terraform/blob/main/tradedataprocessing-setup.sh"><code>tradedataprocessing-setup.sh</code></a>: Script de inicializació de la máquina virtual para la instalación de Docker y otros componentes necesarios (p.e. <code>curl</code>, <code>make</code>).</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">#!/bin/bash

echo "Add Docker's official GPG key"
apt-get update
apt-get install -y ca-certificates curl make
install -m 0755 -d /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
chmod a+r /etc/apt/keyrings/docker.asc

echo "Add the repository to Apt sources"
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release &amp;&amp; echo "$VERSION_CODENAME") stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null
apt-get update

echo "Install Docker packages"
apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

usermod -aG docker ubuntu
systemctl enable docker

exit 0</code></pre>
</div>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>Para crear la máquina virtual en OpenStack, ejecutamos los siguientes comandos:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ terraform init
$ terraform apply</code></pre>
</div>
</div>
<div class="paragraph">
<p>Tras unos minutos, la máquina virtual estará creada y configurada con Docker y el resto de componentes necesarios para instalar la plataforma de data streaming y visualización de Finnhub Stock.</p>
</div>
</div>
<div class="sect2">
<h3 id="_creación_de_la_plataforma_de_data_streaming_y_visualización_con_docker">2.2. Creación de la plataforma de data streaming y visualización con Docker</h3>
<div class="paragraph">
<p>Una vez creada la máquina virtual en OpenStack, podemos proceder a instalar la plataforma de data streaming y visualización de Finnhub Stock con Docker. Para ello, utilizaremos Docker Compose para definir y ejecutar los servicios necesarios para la plataforma. El código de Docker Compose se encuentra en este <a href="https://github.com/ualmtorres/finnhub-data-streaming-docker-compose">repositorio de GitHub</a>. En el archivo <code>docker-compose.yml</code> se definen los servicios necesarios para la plataforma:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Kafka</strong></p>
<div class="ulist">
<ul>
<li>
<p><strong>Zookeeper</strong>: Servidor de coordinación distribuida para *Apache Kafka.</p>
</li>
<li>
<p><strong>Kafka</strong>: Servidor de mensajería para enviar los datos de *las acciones al componente Spark Streaming.</p>
</li>
<li>
<p><strong>Kafdrop</strong>: Interfaz web para visualizar los temas y los mensajes de Kafka.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Spark</strong></p>
<div class="ulist">
<ul>
<li>
<p><strong>Spark Master</strong>: Servidor maestro de Apache Spark.</p>
</li>
<li>
<p><strong>Spark Worker</strong>: Servidor esclavo de Apache Spark.</p>
</li>
<li>
<p><strong>Spark History Server</strong>: Servidor de historial de Apache Spark para visualizar los trabajos y las etapas de Spark.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Cassandra</strong></p>
<div class="ulist">
<ul>
<li>
<p><strong>Cassandra</strong>: Cluster de Cassandra para almacenar los datos de las acciones.</p>
</li>
<li>
<p><strong>Cassandra init</strong>: Componenete de inicialización de Cassandra para crear las tablas necesarias.</p>
</li>
<li>
<p><strong>Cassandra Web</strong>: Interfaz web para visualizar y gestionar la base de datos Cassandra.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Grafana</strong>: Servidor de Grafana para visualizar los datos de las acciones en tiempo real.</p>
</li>
<li>
<p><strong>Jupyter</strong>: Servidor de Jupyter para ejecutar y visualizar el código Python de los componentes de la plataforma.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>La mayoría de estos componentes se comunicarán mediante su nombre de servicio en la red interna de Docker. Sin embargo, hay algunos componentes que necesitan hacerlo a través de la dirección IP de la máquina virtual. Para ello, definiremos la dirección IP de la máquina virtual en una variable de entorno en el archivo <code>.env</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">EXTERNAL_IP= "********"</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>El script de inicialización de la máquina virtual ha creado un archivo <code>.env</code> con la dirección IP flotante de la máquina virtual. El archivo se encuentra en el directorio <code>/home/ubuntu</code> de la máquina virtual. A la hora de ejecutar Docker Compose, se podrá usar ese archivo para definir la dirección IP de la máquina virtual en los servicios que lo necesiten (p.e. Kafka y Cassandra Web).</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Para algunos servicios, como Zookeper, Kafka, Kafdrop, Cassandra, Cassandra Web, Cassandra init y Grafana, usaremos imágenes de Docker que se encuentran en el Docker Hub. Sin embargo, para los servicios de Spark y Jupyter, construiremos las imágenes en el momento de la ejecución. Para ello, definimos los Dockerfiles correspondientes. Todos los archivos necesarios para construir las imágenes de Docker se encuentran en el <a href="https://github.com/ualmtorres/finnhub-data-streaming-docker-compose">repositorio</a>.</p>
</div>
<div class="paragraph">
<p>Para ejecutar la plataforma de data streaming y visualización de Finnhub Stock, utilizamos el siguiente comando cargando las variables de entorno del archivo <code>/home/ubuntu/.env</code>. Este archivo contiene la dirección IP de la máquina virtual que necesitan algunos servicios para comunicarse entre sí y ha sido creado por el script de inicialización de la máquina virtual.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ docker-compose up -d --env-file /home/ubuntu/.env</code></pre>
</div>
</div>
<div class="paragraph">
<p>Tras unos minutos, la plataforma estará desplegada y lista para su uso. Docker Compose habrá creado los siguientes servicios en la máquina virtual. Algunos de ellos serán accesibles a través de la dirección IP de la máquina virtual en los puertos correspondientes. Otros no es necesario que sean accesibles desde el exterior, y simplemente se comunicarán entre sí por la red interna de Docker. Y otros, simplemente habrán sido creados para la configuración de la plataforma. A continuación, se muestra un resumen de los servicios creados:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Kafka</strong></p>
<div class="ulist">
<ul>
<li>
<p><strong>Zookeeper</strong>: No es necesario acceder a este servicio directamente por lo que no se tiene que exponer ningún puerto.</p>
</li>
<li>
<p><strong>Kafka</strong>: Instalación de un solo nodo. No es necesario acceder a este servicio directamente por lo que no se tiene que exponer ningún puerto.</p>
</li>
<li>
<p><strong>Kafdrop</strong>: Accesible a través de la dirección IP de la máquina virtual en el puerto 19000.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Spark</strong></p>
<div class="ulist">
<ul>
<li>
<p><strong>Spark Master</strong>: Accesible a través de la dirección IP de la máquina virtual en el puerto 8080.</p>
</li>
<li>
<p><strong>Spark Worker</strong>: No es necesario acceder a este servicio directamente por lo que no se tiene que exponer ningún puerto.</p>
</li>
<li>
<p><strong>Spark History Server</strong>: Accesible a través de la dirección IP de la máquina virtual en los puertos 18080 y 19888.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Cassandra</strong></p>
<div class="ulist">
<ul>
<li>
<p><strong>Cassandra</strong>: Instalación de tres nodos en modo cluster. No es necesario acceder a este servicio directamente por lo que no se tiene que exponer ningún puerto.</p>
</li>
<li>
<p><strong>Cassandra init</strong>: Su cometido es inicializar la base de datos Cassandra. Una vez finalizada su tarea, se detiene automáticamente. Por tanto, no tiene que exponer ningún puerto.</p>
</li>
<li>
<p><strong>Cassandra Web</strong>: Accesible a través de la dirección IP de la máquina virtual en el puerto 4000.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Grafana</strong>: Accesible a través de la dirección IP de la máquina virtual en el puerto 3000.</p>
</li>
<li>
<p><strong>Jupyter</strong>: Accesible a través de la dirección IP de la máquina virtual en el puerto 443.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>De esta forma, podremos acceder a los servicios de la plataforma a través de la dirección IP de la máquina virtual en los puertos correspondientes:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Grafana: <a href="http://EXTERNAL_IP:3000" class="bare">http://EXTERNAL_IP:3000</a></p>
</li>
<li>
<p>Spark Master: <a href="http://EXTERNAL_IP:8080" class="bare">http://EXTERNAL_IP:8080</a></p>
</li>
<li>
<p>Kafdrop: <a href="http://EXTERNAL_IP:19000" class="bare">http://EXTERNAL_IP:19000</a></p>
</li>
<li>
<p>Cassandra Web: <a href="http://EXTERNAL_IP:4000" class="bare">http://EXTERNAL_IP:4000</a></p>
</li>
<li>
<p>Jupyter: <a href="https://EXTERNAL_IP:443" class="bare">https://EXTERNAL_IP:443</a></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Con estos componentes, podremos crear una plataforma completa para visualizar los precios en tiempo real de las acciones de una empresa. La plataforma nos permitirá realizar análisis de datos sobre las acciones y tomar decisiones informadas sobre inversiones financieras.</p>
</div>
</div>
<div class="sect2">
<h3 id="_configuración_inicial_de_la_plataforma">2.3. Configuración inicial de la plataforma</h3>
<div class="paragraph">
<p>Una vez desplegada la plataforma, necesitaremos realizar algunas configuraciones iniciales para empezar a utilizarla. A continuación, se detallan las configuraciones iniciales necesarias para cada uno de los componentes de la plataforma.</p>
</div>
<div class="sect3">
<h4 id="_configuración_de_kafka">2.3.1. Configuración de Kafka</h4>
<div class="paragraph">
<p>Kafka es un servidor de mensajería que utilizaremos para enviar los datos de las acciones al componente Spark Streaming. Para configurar Kafka, necesitaremos crear un tema en el servidor de Kafka y configurar el productor de datos para enviar los datos al tema. La creación del tema la haremos con comandos en el contenedor de Kafka. A continuación, muestra cómo crear el tema <code>market</code> en Kafka.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ docker exec kafka \
kafka-topics --bootstrap-server kafka:9092 \
             --create \
             --topic market</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_configuración_de_jupyter">2.3.2. Configuración de Jupyter</h4>
<div class="paragraph">
<p>Para ejecutar los productores, consumidores y procesadores de datos de la plataforma, utilizaremos Jupyter. En concreto, lanzaremos todos los scripts desde la terminal de Jupyter. En nuestro caso, trabajaremos con Python 3.10. Para facilitar la gestión de los entornos de Python, utilizaremos Anaconda. A continuación, se muestra cómo instalar Anaconda en el contenedor de Jupyter.</p>
</div>
<div class="paragraph">
<p>Desde la terminal de Jupyter, ejecutamos los siguientes comandos:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ curl https://repo.anaconda.com/archive/Anaconda3-2024.02-1-Linux-x86_64.sh -o anaconda.sh
$ bash anaconda.sh</code></pre>
</div>
</div>
<div class="paragraph">
<p>En el proceso de instalación, aceptamos los términos de la licencia y elegimos la ubicación de la instalación aceptando los valores predeterminados. Una vez finalizada la instalación, nos pedirá si queremos inicializar Anaconda. Aceptamos la inicialización y, una vez finalizada, cargamos el entorno de Anaconda con el siguiente comando:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ conda init bash
$ source ~/.bashrc</code></pre>
</div>
</div>
<div class="paragraph">
<p>A continuación creamos un entorno de Anaconda con Python 3.10 y lo activamos:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ conda create -n python3_10 python=3.10
$ conda activate python3_10</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_desarrollo_de_la_plataforma">3. Desarrollo de la plataforma</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Una vez configurada la plataforma, podemos empezar a desarrollar los componentes necesarios para enviar, procesar y visualizar los datos de las acciones de una empresa. En los siguientes pasos, veremos cómo desarrollar los componentes de la plataforma.</p>
</div>
<div class="sect2">
<h3 id="_creación_de_un_productor_de_datos_de_finnhub_stock">3.1. Creación de un productor de datos de Finnhub Stock</h3>
<div class="paragraph">
<p>Para enviar los datos de las acciones al servidor de Kafka, necesitaremos crear un productor de datos que consuma la API de Finnhub Stock y envíe los datos al tema <code>market</code> de Kafka. El productor de datos lo programaremos en Python y lo ejecutaremos desde el entorno <code>python3_10</code> de Anaconda. A continuación, se muestra un ejemplo de código para un productor de datos de Finnhub Stock en Python. El código del productor se puede encontrar en la carpeta <code>producer</code> de <a href="https://github.com/ualmtorres/finnhub-data-streaming-workload-simulator">este repositorio de GitHub</a>.</p>
</div>
<div class="listingblock">
<div class="title">Archivo <a href="https://github.com/ualmtorres/finnhub-data-streaming-workload-simulator/blob/main/producer/requirements.txt"><code>requirements.txt</code></a> con las dependencias necesarias para el productor de datos.</div>
<div class="content">
<pre class="highlight"><code>websockets
websocket-client
finnhub-python
kafka-python
avro</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Archivo <a href="https://github.com/ualmtorres/finnhub-data-streaming-workload-simulator/blob/main/producer/config.json"><code>config.json</code></a> de configuración con la clave de la API de Finnhub Stock.</div>
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">{
  "KAFKA_SERVER": "kafka",
  "KAFKA_PORT": "29092",
  "KAFKA_TOPIC_NAME": "market",
  "FINNHUB_API_KEY": "**********" <b class="conum">(1)</b>
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Clave de la API de Finnhub Stock.</p>
</li>
</ol>
</div>
<div class="listingblock">
<div class="title">Archivo <a href="https://github.com/ualmtorres/finnhub-data-streaming-workload-simulator/blob/main/producer/producer.py"><code>producer.py</code></a> productor de datos de Finnhub Stock en Python.</div>
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import os
import websocket
import json
import io
import avro.schema
import avro.io

from kafka import KafkaProducer

# https://finnhub.io/docs/api/websocket-trades
class FinnhubProducer:
    def __init__(self):
        """
        Producer class that connects to the finnhub websocket, encodes &amp; validates the JSON payload
        in avro format against pre-defined schema then sends data to kafka.
        """

        # define config from config file
        self.config = self.load_config('config.json')

        # define the kafka producer here. This assumes there is a kafka server already setup at the address and port
        #self.producer = KafkaProducer(bootstrap_servers=f"{self.config['KAFKA_SERVER']}:{self.config['KAFKA_PORT']}",api_version=(0, 10, 1))


        kafka_servers=[config['KAFKA_SERVER'] + config['KAFKA_PORT']]
        self.producer = KafkaProducer(bootstrap_servers = kafka_servers,api_version=(0, 10, 1))

        # define the avro schema here. This assumes the schema is already defined in the src/schemas folder
        # this helps us enforce the schema when we send data to kafka
        self.avro_schema = avro.schema.parse(open('trades.avsc').read())
        print("AVRO schema loaded")

        # define the websocket client
        self.ws = websocket.WebSocketApp(f"wss://ws.finnhub.io?token={self.config['FINNHUB_API_KEY']}",
                                         on_message=self.on_message,
                                         on_error=self.on_error,
                                         on_close=self.on_close)
        self.ws.on_open = self.on_open
        self.ws.run_forever()

    def load_config(self, config_file):
        with open(config_file, 'r') as f:
            config = json.load(f)
        return config

    def avro_encode(self, data, schema):
        """
        Avro encode data using the provided schema.

        Parameters
        ----------
        data : dict
            Data to encode.
        schema : avro.schema.Schema
            Avro schema to use for encoding.

        Returns
        -------
        bytes : Encoded data.
        """

        writer = avro.io.DatumWriter(schema)
        bytes_writer = io.BytesIO()
        encoder = avro.io.BinaryEncoder(bytes_writer)
        writer.write(data, encoder)
        return bytes_writer.getvalue()

    def on_message(self, ws, message):
        """
        Callback function that is called when a message is received from the websocket.

        Parameters
        ----------
        ws : websocket.WebSocketApp
            Websocket client.
        message : str
            Message received from the websocket.
        """
        message = json.loads(message)
        avro_message = self.avro_encode(
            {
                'data': message['data'],
                'type': message['type']
            },
            self.avro_schema
        )
        self.producer.send(self.config['KAFKA_TOPIC_NAME'], avro_message)

    def on_error(self, ws, error):
        """
        Websocket error callback. This currently just prints the error to the console.
        In a production environment, this should be logged to a file or sent to a monitoring service.

        Parameters
        ----------
        ws : websocket.WebSocketApp
            Websocket client.
        error : str
            Error message.
        """
        print(error)

    def on_close(self, ws):
        """
        Websocket close callback. This currently just prints a message to the console.
        In a production environment, this should be logged to a file or sent to a monitoring service.

        Parameters
        ----------
        ws : websocket.WebSocketApp
            Websocket client.
        """
        print("### closed ###")

    def on_open(self, ws):
        """
        Websocket open callback. This subscribes to the MSFT stock topic on the websocket.

        Parameters
        ----------
        ws : websocket.WebSocketApp
            Websocket client.
        """
        print("sending subscribe message")
        self.ws.send('{"type":"subscribe","symbol":"BINANCE:BTCUSDT"}')
        print("subscribed to AAPL")


if __name__ == "__main__":
    FinnhubProducer()</code></pre>
</div>
</div>
<div class="paragraph">
<p>Para ejecutar el productor de datos, haremos lo siguiente:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Activamos el entorno de Anaconda con Python 3.10.</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ conda activate python3_10</code></pre>
</div>
</div>
</li>
<li>
<p>Instalamos las dependencias necesarias.</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ pip install -r requirements.txt</code></pre>
</div>
</div>
</li>
<li>
<p>Ejecutamos el productor de datos.</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ python producer.py</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>El productor de datos se conectará a la API de Finnhub Stock y enviará los datos de las acciones al servidor de Kafka. Podremos ver los mensajes en el tema <code>market</code> de Kafka utilizando la interfaz web de Kafdrop. La figura siguiente muestra un ejemplo de mensajes en el tema <code>market</code> de Kafka.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/kafdrop.png" alt="Kafdrop">
</div>
</div>
<div class="paragraph">
<p>Con el productor de datos en funcionamiento, los datos de Finnhub Stock se están enviando ya al servidor de Kafka para su procesamiento posterior por el componente Spark Streaming. Antes de pasar a crear el componente de Spark Streaming, crearemos un pequeño consumidor de datos para verificar que los datos se están enviando correctamente al servidor de Kafka.</p>
</div>
</div>
<div class="sect2">
<h3 id="_creación_de_un_consumidor_de_datos_de_kafka">3.2. Creación de un consumidor de datos de Kafka</h3>
<div class="paragraph">
<p>Para verificar que los datos se están enviando correctamente al servidor de Kafka, crearemos un consumidor de datos que lea los mensajes del tema <code>market</code> de Kafka. El consumidor de datos lo programaremos en Python y lo ejecutaremos desde el entorno <code>python3_10</code> de Anaconda. El código del consumidor de datos se puede encontrar en la carpeta <code>consumer</code> de <a href="https://github.com/ualmtorres/finnhub-data-streaming-workload-simulator">este repositorio de GitHub</a>.</p>
</div>
<div class="listingblock">
<div class="title">Archivo <a href="https://github.com/ualmtorres/finnhub-data-streaming-workload-simulator/blob/main/consumer/requirements.txt"><code>requirements.txt</code></a> con las dependencias necesarias para el consumidor de datos.</div>
<div class="content">
<pre class="highlight"><code>websockets
websocket-client
finnhub-python
kafka-python
avro</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Archivo <a href="https://github.com/ualmtorres/finnhub-data-streaming-workload-simulator/blob/main/consumer/consumer.py"><code>consumer.py</code></a> consumidor de datos de Kafka en Python.</div>
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">from kafka import KafkaConsumer
import json
import io
import avro.io
import avro.schema

with open('config.json', 'r') as f:
    config = json.load(f)

# define the consumer to read from the Kafka topic

# define kafka servers from the config file (KAFKA_SERVER variable)
kafka_servers=[config['KAFKA_SERVER'] + config['KAFKA_PORT']]
consumer = KafkaConsumer(
    config['KAFKA_TOPIC_NAME'],
    bootstrap_servers = kafka_servers,
    api_version=(0, 10, 1)
    )

# define the Avro schema that corresponds to the encoded data
schema = avro.schema.parse(open('trades.avsc').read())

for message in consumer:
    # asssume 'byte_string' contains the Avro-encoded byte string,
    # we need to decode it using avro library
    bytes_reader = io.BytesIO(message.value)
    decoder = avro.io.BinaryDecoder(bytes_reader)
    reader = avro.io.DatumReader(schema)
    data = reader.read(decoder)
    print(data)</code></pre>
</div>
</div>
<div class="paragraph">
<p>También necesitaremos el archivo <code>config.json</code> con la configuración del consumidor de datos.</p>
</div>
<div class="listingblock">
<div class="title">Archivo <a href="https://github.com/ualmtorres/finnhub-data-streaming-workload-simulator/blob/main/consumer/config.json"><code>config.json</code></a> de configuración del consumidor de datos.</div>
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">{
  "KAFKA_SERVER": "kafka",
  "KAFKA_PORT": "29092",
  "KAFKA_TOPIC_NAME": "market"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Por último, también necesitaremos el archivo <code>trades.avsc</code> con el esquema Avro que corresponde a los datos codificados.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>Avro es un sistema de serialización de datos que proporciona un esquema para la codificación de los datos muy utilizado en el contexto de Apache Hadoop y Apache Kafka. El esquema Avro define la estructura de los datos codificados y permite a los consumidores de datos decodificar los mensajes correctamente.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="title">Archivo <a href="https://github.com/ualmtorres/finnhub-data-streaming-workload-simulator/blob/main/consumer/trades.avsc"><code>trades.avsc</code></a> con el esquema Avro para los datos codificados.</div>
<div class="content">
<pre class="highlight"><code>{
  "type" : "record",
  "name" : "message",
  "namespace" : "FinnhubProducer",
  "fields" : [ {
    "name" : "data",
    "type" : {
      "type" : "array",
      "items" : {
        "type" : "record",
        "name" : "data",
        "fields" : [ {
          "name" : "c",
          "type":[
            {
               "type":"array",
               "items":["null","string"],
               "default":[]
            },
            "null"
          ],
          "doc" : "Trade conditions"
        },
        {
          "name" : "p",
          "type" : "double",
          "doc" : "Price at which the stock was traded"
        },
        {
          "name" : "s",
          "type" : "string",
          "doc" : "Symbol of a stock"
        },
        {
          "name" : "t",
          "type" : "long",
          "doc" : "Timestamp at which the stock was traded"
        },
        {
          "name" : "v",
          "type" : "double",
          "doc" : "Volume at which the stock was traded"
        } ]
      },
      "doc" : "Trades messages"
    },
    "doc"  : "Contains data inside a message"
  },
  {
    "name" : "type",
    "type" : "string",
    "doc"  : "Type of message"
  } ],
  "doc" : "A schema for upcoming Finnhub messages"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>El esquema Avro debe coincidir con el esquema utilizado por el productor de datos para codificar los mensajes. De lo contrario, el consumidor de datos no podrá decodificar los mensajes correctamente.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>Para crear el archivo <code>trades.avsc</code>, puedes utilizar una herramienta como <a href="https://konbert.com/convert/json/to/avro">Konbert</a> para convertir un JSON en un esquema Avro. Basta con subir un archivo JSON con una muestra de los datos codificados, seleccionar que sólo se quiere obtener el esquema y la herramienta generará el esquema Avro correspondiente en un archivo de texto con extensión <code>.avsc</code>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Para ejecutar el consumidor de datos, haremos lo siguiente:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Activamos el entorno de Anaconda con Python 3.10.</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ conda activate python3_10</code></pre>
</div>
</div>
</li>
<li>
<p>Instalamos las dependencias necesarias.</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ pip install -r requirements.txt</code></pre>
</div>
</div>
</li>
<li>
<p>Ejecutamos el consumidor de datos.</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ python consumer.py</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>El consumidor de datos leerá los mensajes del tema <code>market</code> de Kafka y los decodificará utilizando el esquema Avro. Podremos ver los mensajes en la consola a medida que se reciban nuevos datos de la API de Finnhub Stock y se envíen al servidor de Kafka. Con el consumidor de datos en funcionamiento, podemos verificar que los datos se están enviando correctamente al servidor de Kafka y que se pueden leer y decodificar correctamente.</p>
</div>
<div class="paragraph">
<p>La figura siguiente muestra un ejemplo de mensajes decodificados del consumidor de datos de Kafka.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/consumer.png" alt="Consumidor de datos">
</div>
</div>
<div class="paragraph">
<p>Con el productor y el consumidor de datos en funcionamiento, podemos confirmar que los datos de Finnhub Stock se están enviando correctamente al servidor de Kafka y que se pueden leer y decodificar correctamente. En el siguiente paso, crearemos el componente Spark Streaming para procesar los datos de las acciones en tiempo real.</p>
</div>
</div>
<div class="sect2">
<h3 id="_creación_de_un_componente_spark_streaming">3.3. Creación de un componente Spark Streaming</h3>
<div class="paragraph">
<p>El componente Spark Streaming procesará los datos de las acciones en tiempo real y calculará estadísticas sobre los precios de las acciones. Utilizaremos Apache Spark para procesar los datos de las acciones y realizar cálculos sobre los precios de las acciones. El componente Spark Streaming leerá los datos del servidor de Kafka, los procesará en tiempo real y los almacenará en la base de datos Cassandra. El código del componente Spark Streaming se puede encontrar en la carpeta <code>spark</code> de <a href="https://github.com/ualmtorres/finnhub-data-streaming-workload-simulator">este repositorio de GitHub</a>. A continuación, se muestra un ejemplo de código para un componente Spark Streaming en Python.</p>
</div>
<div class="listingblock">
<div class="title">Archivo <a href="https://github.com/ualmtorres/finnhub-data-streaming-workload-simulator/blob/main/spark/requirements.txt"><code>requirements.txt</code></a> con las dependencias necesarias para el componente Spark Streaming.</div>
<div class="content">
<pre class="highlight"><code>pyspark==3.2.1
kafka-python
cassandra-driver</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Archivo <a href="https://github.com/ualmtorres/finnhub-data-streaming-workload-simulator/blob/main/spark/main.py"><code>main.py</code></a> del componente Spark Streaming en Python.</div>
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.avro.functions import from_avro
from pyspark.sql.streaming import *

from uuid import uuid1

# create a Spark session
spark = SparkSession \
    .builder \
    .master("spark://master:7077") \
    .appName("StreamProcessor") \
    .config("spark.cassandra.connection.host", 'cassandra1') \
    .config("spark.cassandra.connection.port", '9042') \
    .config("spark.cassandra.auth.username", 'cassandra') \
    .config("spark.cassandra.auth.password", 'cassandra') \
    .getOrCreate()

# suppress all the INFO logs except for errors
spark.sparkContext.setLogLevel("ERROR")

# define the Avro schema that corresponds to the encoded data
tradesSchema = open('./trades.avsc', 'r').read()

@udf(returnType=StringType())
def makeUUID():
    return str(uuid1())

# define the stream to read from the Kafka topic market
inputDF = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:29092") \
    .option("subscribe", "market") \
    .option("minPartitions", "1") \
    .option("maxOffsetsPerTrigger", "1000") \
    .option("useDeprecatedOffsetFetching", "false") \
    .load()



# explode the data column and select the columns we need
expandedDF = inputDF \
    .withColumn("avroData", from_avro(col("value"), tradesSchema)) \
    .select(col("avroData.*")) \
    .select(explode(col("data")), col("type")) \
    .select(col("col.*"), col("type"))

# create the final dataframe with the columns we need plus the ingest timestamp
finalDF = expandedDF \
    .withColumn("uuid", makeUUID()) \
    .withColumnRenamed("c", "trade_conditions") \
    .withColumnRenamed("p", "price") \
    .withColumnRenamed("s", "symbol") \
    .withColumnRenamed("t", "trade_timestamp") \
    .withColumnRenamed("v", "volume") \
    .withColumn("trade_timestamp", (col("trade_timestamp") / 1000).cast("timestamp")) \
    .withColumn("ingest_timestamp", current_timestamp().alias("ingest_timestamp"))

# write the final dataframe to Cassandra
# spark handles the streaming and batching for us
query = finalDF \
    .writeStream \
    .trigger(processingTime="5 seconds") \
    .foreachBatch(lambda batchDF, batchId: \
        batchDF.write \
            .format("org.apache.spark.sql.cassandra") \
            .option("table", "trades") \
            .option("keyspace", "market") \
            .mode("append") \
            .save()) \
    .outputMode("update") \
    .start()

# create a summary dataframe with the average price * volume
summaryDF = finalDF \
    .withColumn("price_volume_multiply", col("price") * col("volume")) \
    .withWatermark("trade_timestamp", "15 seconds") \
    .groupBy("symbol") \
    .agg(avg("price_volume_multiply").alias("price_volume_multiply"))

# add UUID and ingest timestamp to the summary dataframe and rename agg column
finalsummaryDF = summaryDF \
    .withColumn("uuid", makeUUID()) \
    .withColumn("ingest_timestamp", current_timestamp().alias("ingest_timestamp")) \
    .withColumnRenamed("avg(price_volume_multiply)", "price_volume_multiply")

# write the summary dataframe to Cassandra in 5 second batches
query2 = finalsummaryDF \
    .writeStream \
    .trigger(processingTime="5 seconds") \
    .foreachBatch(lambda batchDF, batchId: \
        batchDF.write \
            .format("org.apache.spark.sql.cassandra") \
            .option("table", "running_averages_15_sec") \
            .option("keyspace", "market") \
            .mode("append") \
            .save()) \
    .outputMode("update") \
    .start()

# wait for the stream to terminate - i.e. wait forever
spark.streams.awaitAnyTermination()</code></pre>
</div>
</div>
<div class="paragraph">
<p>Para ejecutar el componente Spark Streaming, haremos lo siguiente:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Activamos el entorno de Anaconda con Python 3.10.</p>
</li>
</ol>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ conda activate python3_10</code></pre>
</div>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Instalamos las dependencias necesarias.</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ pip install -r requirements.txt</code></pre>
</div>
</div>
</li>
<li>
<p>Ejecutamos el componente Spark Streaming.</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ spark-submit \
    --master spark://&lt;&lt;SPARK_MASTER&gt;&gt;:7077 \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1,\
    org.apache.spark:spark-avro_2.12:3.2.1,\
    com.datastax.spark:spark-cassandra-connector_2.12:3.2.0 \
    main.py</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>El repositorio contiene un archivo bash <code>run.sh</code> que facilita la ejecución del componente Spark Streaming. El archivo <code>run.sh</code> contiene el comando <code>spark-submit</code> con las opciones necesarias para ejecutar el componente Spark Streaming. Para ejecutar el componente Spark Streaming, simplemente ejecutamos el archivo <code>run.sh</code> desde la terminal (p.e. <code>bash run.sh</code> si estamos en la terminal de Jupyter).</p>
</div>
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>El componente Spark Streaming leerá los datos del servidor de Kafka, los procesará en tiempo real y los almacenará en la base de datos Cassandra. El resultado de la ejecución del componentes de Spark Streaming se puede consultar en la interfaz web de Spark History Server en la dirección <a href="http://EXTERNAL_IP:18080" class="bare">http://EXTERNAL_IP:18080</a>. Podremos ver los trabajos y las etapas de Spark en la interfaz web de Spark History Server. La figura siguiente muestra un ejemplo de la interfaz web de Spark History Server. Al seleccionar un trabajo o una etapa, podremos ver los detalles del trabajo o la etapa, incluidos los registros y las estadísticas de ejecución. Podremos utilizar la interfaz web de Spark History Server para depurar y optimizar los trabajos y las etapas de Spark.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/sparkhistoryserver.png" alt="Spark History Server">
</div>
</div>
<div class="paragraph">
<p>Al mismo tiempo, los datos procesados se están almacenando en la base de datos Cassandra. Podremos ver los datos procesados en la base de datos Cassandra utilizando Cassandra Web. La figura siguiente muestra un ejemplo de datos almacenados en la base de datos Cassandra.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/cassandraweb.png" alt="Cassandra Web">
</div>
</div>
<div class="paragraph">
<p>Con el componente Spark Streaming en funcionamiento, los datos de las acciones se están procesando en tiempo real y se están almacenando en la base de datos Cassandra. Podremos realizar consultas sobre los datos almacenados en Cassandra y visualizar los resultados en Cassandra Web. En el siguiente paso, veremos los datos procesados en la interfaz web de Grafana mediante un dashboard interactivo.</p>
</div>
</div>
<div class="sect2">
<h3 id="_visualización_de_los_datos_en_grafana">3.4. Visualización de los datos en Grafana</h3>
<div class="paragraph">
<p>Grafana es una plataforma de visualización de datos que utilizaremos para visualizar los datos de las acciones en tiempo real. Utilizaremos Grafana para crear un dashboard interactivo que muestre los precios de las acciones y las estadísticas calculadas por el componente Spark Streaming. El dashboard mostrará gráficos en tiempo real de los precios de las acciones y las medias móviles de los precios de las acciones. El dashboard se cargó en Grafana añadiéndole un archivo de configuración en formato JSON (<a href="https://github.com/ualmtorres/finnhub-data-streaming-docker-compose/blob/main/grafana/dashboards/dashboard.json"><code>dashboard.json</code></a>) al crear la imagen de Grafana que se está usando en este tutorial. Se trata de un dashboard de ejemplo con tres paneles: uno para mostrar los precios de las acciones, otro para mostrar el volumen de las acciones y otro para mostrar las medias móviles de los precios de las acciones en un intervalo de 15 segundos.</p>
</div>
<div class="paragraph">
<p>Para acceder al dashboard en Grafana, abrimos un navegador web y vamos a la dirección <a href="http://EXTERNAL_IP:3000" class="bare">http://EXTERNAL_IP:3000</a>. El dashboard se muestra de forma predeterminada en la página principal de Grafana. Podremos ver los gráficos en tiempo real de los precios de las acciones y las medias móviles de los precios de las acciones en el dashboard. La figura siguiente muestra un ejemplo de un dashboard interactivo en Grafana.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/dashboard.png" alt="Grafana">
</div>
</div>
<div class="paragraph">
<p>Con el dashboard en funcionamiento, podremos visualizar los precios de las acciones y las estadísticas calculadas por el componente Spark Streaming en tiempo real. Podremos realizar análisis de datos sobre las acciones y tomar decisiones informadas sobre inversiones financieras.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_conclusiones">4. Conclusiones</h2>
<div class="sectionbody">
<div class="paragraph">
<p>En este tutorial hemos creado una plataforma de data streaming y visualización de Finnhub Stock. Se trata de una plataforma completa para visualizar los precios en tiempo real de las acciones de una empresa. Hemos utilizado tecnologías de Big Data como Apache Kafka, Apache Spark, Apache Cassandra, Grafana y Jupyter para procesar los datos de las acciones en tiempo real y visualizarlos en un dashboard interactivo. Tambien se ha utilizado Infraestructura como Código (IaC) con Terraform para crear la máquina virtual en OpenStack, mostrando cómo se puede automatizar el despliegue de la plataforma en un entorno cloud. Además, hemos utilizado contenedores, concretamente Docker, para desplegar los componentes de la plataforma, mostrando cómo se puede empaquetar y desplegar aplicaciones de forma sencilla y portable. Con esta plataforma podremos realizar análisis de datos sobre las acciones y tomar decisiones sobre inversiones financieras basadas en los datos.</p>
</div>
</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
Last updated 2024-04-18 17:49:10 +0200
</div>
</div>
</body>
</html>